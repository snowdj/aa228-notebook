{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"gridworld.jl\")\n",
    "g = DMUGridWorld();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply Q-learning from Algorithm 5.3 in the text. We'll train over 1000 100-step runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qlearn (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Qlearn(g, alpha, epsilon)\n",
    "    # initialize dictionary\n",
    "    Q = Dict{Int, Vector{Float64}}()\n",
    "    \n",
    "    # initialize Q-values at initial state (s = 1)\n",
    "    Q[1] = zeros(n_actions(g))\n",
    "    \n",
    "    # 1000 simulations\n",
    "    for k = 1:1000\n",
    "        s = 1\n",
    "        for t = 0:100\n",
    "            # choose a based on Q and some exploration strategy\n",
    "            a_idx = findmax(Q[s])[2]\n",
    "            if rand() < epsilon\n",
    "                a_idx = rand(1:4)\n",
    "            end\n",
    "            a = actions(g)[a_idx]\n",
    "\n",
    "            # observe new state s_{t+1} and reward rt\n",
    "            sp, r = simulate(g, s, a)\n",
    "\n",
    "            # if we've never observed this state, initialize it to zeros\n",
    "            if !haskey(Q, sp)\n",
    "                Q[sp] = zeros(n_actions(g))\n",
    "            end\n",
    "\n",
    "            # update Q values\n",
    "            Q[s][a_idx] += alpha * ( r + discount(g)*maximum(Q[sp]) - Q[s][a_idx] )\n",
    "\n",
    "            # update s\n",
    "            s = sp\n",
    "            \n",
    "            # 73 and 88 are terminal states. Just quit if we get in them.\n",
    "            if s == 73 || s == 88\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return Q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Qlearn(g, 0.5, 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the Q-learning work? Let's compare it to a random policy during 10 simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learned policy: -898\n",
      "random poilcy:    -1187629\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "Random.seed!(1)     # for reproducibility, seed random number generator\n",
    "\n",
    "r_sum = 0.0      # sum for policy from Q-learning\n",
    "rr_sum = 0.0     # sum for random policy\n",
    "\n",
    "# run 10 simulations\n",
    "for k = 1:10\n",
    "    s = 1    # initial state for policy from Q-learning\n",
    "    sr = 1   # initial state for random policy\n",
    "    \n",
    "    for t = 0:100\n",
    "        \n",
    "        # generate actions for both policies\n",
    "        a = actions(g)[findmax(Q[s])[2]]\n",
    "        ar = actions(g)[rand(1:4)]\n",
    "        \n",
    "        # advance Q simulation if you aren't in a terminal state\n",
    "        if s != 73 && s != 88\n",
    "            sp, r = simulate(g, s, a)\n",
    "            r_sum += r * discount(g) ^ (-t)\n",
    "            s = sp\n",
    "        end\n",
    "        \n",
    "        # advance random simulation if you aren't in a terminal state\n",
    "        if sr != 73 && sr != 88\n",
    "            spr, rr = simulate(g, sr, ar)\n",
    "            rr_sum += rr * discount(g) ^ (-t)\n",
    "            sr = spr\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Q-learned policy: \", round(Int, r_sum))\n",
    "println(\"random poilcy:    \", round(Int, rr_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative sum from Q-learning is much better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
